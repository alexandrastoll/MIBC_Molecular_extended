{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a54d6c-6cdd-408d-bdf6-fa9b7437eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "##helper functions\n",
    "import import_ipynb\n",
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "\n",
    "\n",
    "from models import ABMIL, DSMIL, MeanPoolingMIL, MeanPoolingMILREG, IClassifier, BClassifier\n",
    "from datasets import WSIFeatDataset, WSIFeatDataset_Reg\n",
    "from helperfunctions import strat_k_fold, sampler_strat_kfold\n",
    "from datasets import WSIFeatDataset, WSIFeatDataset_Reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070112be-38a2-4b6d-a800-6448ad38220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "in_dim = 1536 # dim of UNI2-h embedding -> if a case has 500 patches extracted, respective case embedding dims are 500 x 1536 (with batch size = 1 -> dims: 1, 500, 1536).\n",
    "\n",
    "mil_types = ['multipoint_regressor', 'dsmil', 'abmil', 'meanpoolmil']\n",
    "\n",
    "accumulate = True\n",
    "test = True\n",
    "feature_dir = \"/path/to/dir\"\n",
    "\n",
    "# feature dir contains one .pt file per case which includes all patch embeddings per case in one file. \n",
    "\n",
    "target_column = [\"combined\"]  # Regression labels: One Column with all three regression values per class for one molecular simplified consensus subtype each.\n",
    "                              # example for one ID: [0.6176298319609687, 0.5754460199978302, 0.5890559926087964]\n",
    "\n",
    "\n",
    "class_column = \"consensusClass\"  # Categorical molecular subtype label for evaluation\n",
    "\n",
    "df_274_reg = pd.read_excel(\"/path/to/df.xlsx\")\n",
    "\n",
    "df_train = df_274_reg\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "runs = 10\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "for mil_type in mil_types:\n",
    "    fold_accuracies = []\n",
    "    fold_auc_scores = []\n",
    "\n",
    "    for z in range(runs):\n",
    "        group_name = f\"{os.path.basename(os.path.normpath(feature_dir))}_{mil_type}_ep:{epochs}\"\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        dfs = sampler_strat_kfold(strat_k_fold(df_train[[\"ID\", \"consensusClass\"]], n_splits=n_splits, random_state=None), rs=\"ros\", random_state=None, random_state_valid=None)\n",
    "\n",
    "        for i in range(n_splits):\n",
    "\n",
    "            wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            project=\"tcga_blca_molecular_uni2\",\n",
    "            group=group_name,\n",
    "            job_type=f\"run{z}\",\n",
    "            config={\"mil_type\": mil_type, \"epochs\": epochs, \"num_classes\": num_classes, \"runs\": runs}\n",
    "            )\n",
    "            \n",
    "            print(f\"Training {mil_type}, Fold {i+1}/5\")\n",
    "\n",
    "            if mil_type == 'multipoint_regressor':\n",
    "\n",
    "                df_oversampled = dfs[i][[\"ID\", f\"split{i}\"]].merge(df_train, on=\"ID\", how= \"left\")\n",
    "            \n",
    "                df_oversampled[\"combined\"] = df_oversampled[[\"LumAll\", \"Ba/Sq\", \"Stroma-rich\"]].values.tolist()\n",
    "                train_data = df_oversampled[df_oversampled[f\"split{i}\"] == False].reset_index(drop=True)[[\"ID\", \"combined\"]]\n",
    "                val_data = df_oversampled[df_oversampled[f\"split{i}\"] == True].reset_index(drop=True)[[\"ID\", \"consensusClass\"]]\n",
    "\n",
    "                \n",
    "                # Load dataset\n",
    "                train_dataset_reg = WSIFeatDataset_Reg(train_data, feature_dir, id_column=\"ID\", target_column=\"combined\", phase=\"train\")\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset_reg, batch_size=1, shuffle=True, num_workers=4)\n",
    "                \n",
    "\n",
    "                val_dataset_reg = WSIFeatDataset_Reg(val_data, feature_dir, id_column=\"ID\", class_column=\"consensusClass\",\n",
    "                                                    phase=\"val\")\n",
    "\n",
    "                val_loader = DataLoader(val_dataset_reg, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "            elif mil_type in ['dsmil', 'abmil', 'meanpoolmil']:\n",
    "                \n",
    "                train_data = dfs[i][dfs[i][f\"split{i}\"] == False].reset_index(drop=True)[[\"ID\", \"consensusClass\"]]\n",
    "                val_data = dfs[i][dfs[i][f\"split{i}\"] == True].reset_index(drop=True)[[\"ID\", \"consensusClass\"]]\n",
    "\n",
    "                train_dataset = WSIFeatDataset(train_data, feature_dir=feature_dir, id_column= \"ID\",\n",
    "                 label_column = \"consensusClass\",)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4) \n",
    "            \n",
    "                val_dataset = WSIFeatDataset(val_data, feature_dir=feature_dir, id_column= \"ID\",\n",
    "                 label_column = \"consensusClass\",)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "            else:\n",
    "                print(\"Something went wrong with selected MIL type.\")\n",
    "                break\n",
    "                \n",
    "\n",
    "\n",
    "            # Initialize model\n",
    "            if mil_type == 'abmil':\n",
    "                model = ABMIL(in_dim, 512, 128, num_classes)\n",
    "            elif mil_type == 'dsmil':\n",
    "                model = DSMIL(IClassifier(in_dim, num_classes), BClassifier(in_dim, num_classes))\n",
    "            elif mil_type == 'meanpoolmil':\n",
    "                model = MeanPoolingMIL(in_dim, 768, num_classes)\n",
    "            elif mil_type == 'multipoint_regressor':\n",
    "                model = MeanPoolingMILREG(in_dim, 768, num_classes)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), 5e-4, weight_decay=5e-4)\n",
    "            scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs, 5e-5)\n",
    "            model = model.to(device)\n",
    "\n",
    "            max_acc = 0.0\n",
    "            balanced_acc = 0.0\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                loss_sum = 0.0\n",
    "                n = 0\n",
    "\n",
    "                loop = tqdm(train_loader, total=len(train_loader), desc=f'Train [{epoch}/{epochs}]')\n",
    "                model.train()\n",
    "\n",
    "                for file_name, features, label in loop:\n",
    "                    label = label.to(device)\n",
    "                    features = features.squeeze(0).to(device)\n",
    "\n",
    "                    if mil_type == 'abmil':\n",
    "                        scores = model(features)\n",
    "                        loss = F.cross_entropy(scores, label)\n",
    "                    elif mil_type == 'dsmil':\n",
    "                        classes, bag_prediction, _, _ = model(features)\n",
    "                        max_prediction, _ = torch.max(classes, 0, True)\n",
    "                        loss_bag = F.cross_entropy(bag_prediction, label)\n",
    "                        loss_max = F.cross_entropy(max_prediction.view(1, -1), label)\n",
    "                        loss = 0.5 * loss_bag + 0.5 * loss_max\n",
    "                    elif mil_type == 'meanpoolmil':\n",
    "                        scores = model(features.unsqueeze(0))\n",
    "                        loss = F.cross_entropy(scores, label)\n",
    "                    elif mil_type == 'multipoint_regressor': \n",
    "                        scores = model(features.unsqueeze(0))  # Predict 3 continuous values\n",
    "                        loss = nn.L1Loss()(scores, label)  # Regression loss\n",
    "\n",
    "                    if accumulate:\n",
    "                        loss = loss / 4\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        if (n + 1) % 4 == 0 or (n + 1) == len(train_loader):\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                    else:\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    n += 1\n",
    "                    loss_sum += loss.item()\n",
    "\n",
    "                    loop.set_postfix(loss=loss.item(), loss_mean=loss_sum / n)\n",
    "\n",
    "                wandb.log({\"epoch\": epoch, \"train_loss\": loss_sum / n})\n",
    "\n",
    "                if test:\n",
    "                    with torch.no_grad():\n",
    "                        correct = 0\n",
    "                        total = 0\n",
    "                        y_true, y_pred, y_probs = [], [], []\n",
    "                \n",
    "                        loop = tqdm(val_loader, total=len(val_loader), desc=f'Val [{epoch}/{epochs}]')\n",
    "                        model.eval()\n",
    "                \n",
    "                        for file_name, features, label in loop:\n",
    "                            label = label.to(device)\n",
    "                            features = features.squeeze(0).to(device)\n",
    "                \n",
    "                            if mil_type == 'abmil':\n",
    "                                scores = model(features)\n",
    "                                scores = torch.softmax(scores, dim=1).cpu()\n",
    "                                class_label = label.cpu().numpy().item()\n",
    "\n",
    "                                \n",
    "                            elif mil_type == 'dsmil':\n",
    "                                classes, bag_prediction, _, _ = model(features)\n",
    "                                max_prediction, _ = torch.max(classes, 0, True)\n",
    "                                scores = (0.5 * torch.softmax(max_prediction, dim=1) + 0.5 * torch.softmax(bag_prediction, dim=1)).cpu()\n",
    "\n",
    "                                class_label = label.cpu().numpy().item()\n",
    "                                \n",
    "                            elif mil_type == 'meanpoolmil':\n",
    "                                scores = model(features.unsqueeze(0))\n",
    "                                scores = torch.softmax(scores, dim=1).cpu()\n",
    "                                class_label = label.cpu().numpy().item()\n",
    "                                \n",
    "                            elif mil_type == 'multipoint_regressor':\n",
    "                                scores = model(features.unsqueeze(0))  # Predict 3 continuous values\n",
    "                                scores = scores.cpu().numpy().flatten()  # Convert to NumPy (shape: [3])\n",
    "                                class_label = label.cpu().numpy().item()\n",
    "\n",
    "                            y_pred.append(scores)# Store raw regression predictions\n",
    "                            y_true.append(class_label)  # Store true categorical label\n",
    "                                                    \n",
    "                            \n",
    "                        y_pred_np = np.vstack(y_pred)  # Shape: [num_samples, 3] (continuous predictions)\n",
    "                        y_true_np = np.array(y_true)  # Shape: [num_samples] (categorical labels)\n",
    "\n",
    "                        y_class_pred_np = np.argmax(y_pred_np, axis=1)  # Convert to categorical predictions\n",
    "                \n",
    "                        # Compute Classification Metrics\n",
    "                        test_accuracy = accuracy_score(y_true_np, y_class_pred_np)\n",
    "                        bal_acc = balanced_accuracy_score(y_true_np, y_class_pred_np)\n",
    "                \n",
    "                        # Compute AUC (macro-averaged, multi-class)\n",
    "                        if mil_type == 'multipoint_regressor':\n",
    "\n",
    "                            # Compute Regression Metrics\n",
    "                            mae = np.mean(np.abs(y_pred_np - y_true_np.reshape(-1, 1)))  # Mean Absolute Error\n",
    "                            mse = np.mean((y_pred_np - y_true_np.reshape(-1, 1)) ** 2)  # Mean Squared Error\n",
    "                    \n",
    "                            print(f\"Validation MAE: {mae:.4f}\")\n",
    "                            print(f\"Validation MSE: {mse:.4f}\")\n",
    "                            \n",
    "                            test_auc = roc_auc_score(y_true_np, softmax(y_pred_np, axis=1), multi_class=\"ovr\", average=\"macro\")\n",
    "                        \n",
    "                        else:\n",
    "                            test_auc = roc_auc_score(y_true_np, y_pred_np, multi_class=\"ovr\", average=\"macro\")\n",
    "                \n",
    "                        print(f\"Validation Accuracy: {test_accuracy:.4f}\")\n",
    "                        print(f\"Validation Balanced Accuracy: {bal_acc:.4f}\")\n",
    "                        print(f\"Validation AUC (Macro-Averaged): {test_auc:.4f}\")                            \n",
    "\n",
    "                    wandb.log({\"epoch\": epoch, \"acc_agg\": test_accuracy, \"auc_macro_agg\": test_auc, \"bal_acc\": bal_acc})\n",
    "\n",
    "                scheduler.step()\n",
    "            \n",
    "            if z >= runs - 1 and i >= n_splits -1:\n",
    "                print(\"logging code:\")\n",
    "                code_artifact = wandb.Artifact(type=\"code\", name=f\"{mil_type}_ep_{epochs}\")\n",
    "                code_artifact.add_file(f\"./{nb_fname}.ipynb\")\n",
    "                wandb.log_artifact(code_artifact)\n",
    "            \n",
    "            wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python default (fastai2)",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
